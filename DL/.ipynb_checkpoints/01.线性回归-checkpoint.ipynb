{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，例如预测房屋价格、气温、销售量等连续的问题。\n",
    "\n",
    "与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。\n",
    "\n",
    "由于线性回归和softmax回归都是单层神经网络，它们涉及的概率和技术同样适用于大多数的深度学习模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **线性回归的基本要素**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1、模型**\n",
    "\n",
    "线性回归假设输出与各个输入之间是线性关系： <font size=4> $\\widehat{y} = x_1w_1 + x_2w_2 + b $</font> ; 其中 $\\widehat{y}$ 是关于输入 $x_1,x_2$ 的输出表达式，即模型； $w_1,w_2$ 是权重(weight)， $b$ 是偏差（bias) ，它们是线性回归模型的参数（parameter)。 模型输出 $\\widehat{y}$ 是线性回归对真实价格 $y$ 的预测或估计。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2、模型的训练**\n",
    "\n",
    "通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫做**模型训练（model training)**\n",
    "\n",
    "**2.1 训练数据**\n",
    "\n",
    "**2.2 损失函数** \n",
    "\n",
    "在模型训练中，我们要衡量预测值与真实值之间的误差。在机器学习中，将衡量误差的函数称为损失函数（loss function)：<font size=4> $L(w_1,w_2,b) = \\frac{1}{2}(\\widehat{y} - y)^2$ </font> \n",
    "\n",
    "在模型训练中，我们希望找到一组模型参数，记作 $w_1^*,w_2^*,b^*$，来使训练样本平均损失最小：\n",
    "#### $$L(w_1^*,w_2^*,b^*) = \\frac{1}{2}(x_1w_1^* + x_2w_2^* + b^* - y)^2$$\n",
    "\n",
    "**2.3 优化算法** \n",
    "\n",
    "当模型和损失函数形式比较简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫做解析解（analytical solution); 然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫做数值解（numerical solution)。\n",
    "\n",
    "在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：\n",
    "\n",
    " （1）先选取一组模型参数的初始值，如随机选取 ；\n",
    "\n",
    " （2）接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch) $\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度）\n",
    "\n",
    " （3）最后由此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。\n",
    " \n",
    "  在线性回归模型中，模型的每个参数将作如下迭代：\n",
    "  #### $$ \\begin{align} & w_1 \\leftarrow w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} \\frac{\\partial L^{(i)}(w_1,w_2,b) }{\\partial w_1} =  w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} x^{(i)}_1\\big(x^{(i)}_1w_1 + x^{(i)}_2w_2 + b - y^{(i)}\\big) \\\\\n",
    "                        & w_2 \\leftarrow w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} \\frac{\\partial L^{(i)}(w_1,w_2,b) }{\\partial w_2} =  w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} x^{(i)}_2\\big(x^{(i)}_1w_1 + x^{(i)}_2w_2 + b - y^{(i)}\\big) \\\\\n",
    "                        & b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} \\frac{\\partial L^{(i)}(w_1,w_2,b) }{\\partial b} =  b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i\\in \\mathcal{B}} \\big(x^{(i)}_1w_1 + x^{(i)}_2w_2 + b - y^{(i)}\\big) \\\\   \\end{align}$$ \n",
    "                        \n",
    "在上式中，**$\\mathcal{B}$ 代表每个小批量中的样本个数（批大小，batch_size), $\\eta$ 称作学习率（learning rate)，并取正数** 。 \n",
    "\n",
    "<font size=2><i><b>注：这里的批量大小和学习率是人为设定的，并不是通过模型训练学出的，因此叫做超参数（hyperparameter)。我们通常所说的\"调参\"指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</b></i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3、模型预测**\n",
    "\n",
    "根据最小化损失函数的最优解 $w_1^*,w_2^*,b$，利用回归模型 $x_1w_1^* + x_2w_2^* + b^*$ 来估计训练数据以外的任何数据。\n",
    "\n",
    "$$ $$ $$ $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **线性回归的表示方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from mxnet import nd,autograd \n",
    "import random \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**1、生成数据集**</font>\n",
    "\n",
    "设训练数据集样本数为1000，输入个数（特征数）为2，给定随机生成的批量样本特征 $X\\in \\mathcal{R}^{1000*2}$，使用线性回归模型真实权重 $w = [2,-3.4]^T$ 和偏差 $b = 4.2$ 以及一个随机噪声项 $\\epsilon$ 来生成标签: $y = Xw + b + \\epsilon $ 其中，噪声 $\\epsilon$ 服从均值为0、标准差为0.01 的正态分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_inputs = 2  # 输入特征个数\n",
    "num_examples = 1000 # 样本数\n",
    "\n",
    "# 真实的权重w 和偏差 b\n",
    "true_w = nd.array([2,-3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "# 生成输入项\n",
    "features = nd.random.normal(scale = 1,shape = (num_examples,num_inputs))\n",
    "\n",
    "labels = nd.dot(features,true_w.T) + b\n",
    "labels += nd.random.normal(scale = 0.01,shape = labels.shape)\n",
    "\n",
    "# 画图 \n",
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(features[:,0].asnumpy(),labels.asnumpy(),label = \"feature0\",color='red')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(features[:,1].asnumpy(),labels.asnumpy(),label = \"feature1\",color='blue')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**2、读取数据**</font>\n",
    "\n",
    "根据batch_size 遍历数据并读取小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size,features,labels):\n",
    "\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "\n",
    "    # 打乱features的读取顺序\n",
    "    random.shuffle(indices)\n",
    "    # print(indices)\n",
    "\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        # print(i,min(i+batch_size,num_examples))\n",
    "        j = nd.array(indices[i:min(i + batch_size , num_examples)])\n",
    "        yield features.take(j),labels.take(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=2><i><b>一个带有 \"yield\" 的函数就是一个generator；它和普通的函数不同，生成一个generator看起来像函数调用，但是不会执行任何函数代码，直到对其调用 next() ( 在 for 循环中会自动调用next() ) 才开始执行。虽然执行流程仍按函数的流程执行，但每执行到一个yield语句就会中断，并返回一个迭代值，下次执行时从yield的下一个语句继续执行。看起来就好像一个函数在正常执行的时候被yield中断了数次，每次都会通过yield返回当前值。\n",
    "yield的好处显而易见，把一个函数改写成一个generator就会获得迭代能力，比起用类的实例保存状态来计算下一个next()的值，不仅代码简洁，而且执行流程异常清晰。 </b></i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_iter at 0x000001E07D4DE410>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10 \n",
    "data_iter(batch_size,features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_i = 0\n",
    "for X,y in data_iter(batch_size,features,labels):\n",
    "    print(\"i=\",temp_i,X,y)\n",
    "    temp_i +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**3、初始化模型参数**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化权重和偏差\n",
    "w = nd.random.normal(scale = 0.01,shape = (num_inputs,1)) # num_inputs 输入的特征个数\n",
    "b = nd.zeros(shape=(1,))\n",
    "\n",
    "# 创建权重和偏差的梯度，之后会对它们求导\n",
    "w.attach_grad()\n",
    "b.attach_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**4、定义模型**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    return nd.dot(X,w) + b \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**5、策略：定义损失函数**</font>\n",
    "\n",
    "使用平方损失来定义线性回归的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat,y): # y_hat：预测值； y: 真实值\n",
    "    return nd.power(y_hat - y.reshape(y_hat.shape) , 2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**6、算法：定义优化算法**</font>\n",
    "\n",
    "使用小批量随机梯度下降算法。通过不断迭代参数来优化损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,lr,batch_size): \n",
    "    '''\n",
    "    params ：需要求导的参数，这里指的是w、b\n",
    "    lr：学习率\n",
    "    batch_size: 小批量的样本数\n",
    "    '''\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>**7、训练模型**</font>\n",
    "\n",
    "多次迭代模型参数。在每次迭代中，根据当前读取的小批量数据样本，通过反向函数 backward 计算小批量样本的梯度，并调用优化算法迭代模型参数。\n",
    "\n",
    "在一个迭代周期（epoch）中，我们将完整遍历一遍 data_iter函数，并对训练数据集中所有样本使用一次。\n",
    "\n",
    "在实践中，大多数超参数都需要通过反复试错来不断调节。虽然迭代周期设得越大模型可能越有效，但是训练时间可能过长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.000173\n",
      "epoch 2 loss 0.000052\n",
      "epoch 3 loss 0.000051\n",
      "epoch 4 loss 0.000051\n",
      "epoch 5 loss 0.000051\n",
      "epoch 6 loss 0.000052\n",
      "epoch 7 loss 0.000052\n",
      "epoch 8 loss 0.000052\n",
      "epoch 9 loss 0.000051\n",
      "epoch 10 loss 0.000051\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05 # 学习率 \n",
    "num_epochs = 10 # 迭代周期\n",
    "net = linreg # 神经元上上面定义的线性模型\n",
    "loss =squared_loss # 损失函数是上面定义的平方损失函数\n",
    "batch_size = 10 # 批大小\n",
    "\n",
    "for epoch in range(num_epochs): # 训练模型一共需要num_epochs 个迭代周期\n",
    "    # 每个迭代周期，都会将训练数据全部训练一次\n",
    "    for X,y in data_iter(batch_size,features,labels) :\n",
    "        with autograd.record(): # 调用 autograd.record 函数来记录与梯度相关的计算\n",
    "            l = loss(net(X,w,b), y ) ## y_hat = lonreg(X,w,b); l = squared_loss(y_hat,y)\n",
    "        \n",
    "        l.backward() # 小批量地损失函数对模型参数求梯度\n",
    "        \n",
    "        sgd( [w,b], lr,batch_size) # 使得小批量随机梯度下降迭代模型参数\n",
    "        \n",
    "    # print(\"epoch:\",epoch+1, w,b)\n",
    "    train_l = loss( net(features,w,b) , labels ) # 根据迭代得到的w,b,计算最终损失函数\n",
    "    print(\"epoch %d loss %f\" % (epoch+1, train_l.mean().asnumpy()))\n",
    "    # print(w,b)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### **简洁实现**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.013017\n",
      "epoch 2 loss 0.000079\n",
      "epoch 3 loss 0.000052\n"
     ]
    }
   ],
   "source": [
    "######################################## 1、生成 数据 \n",
    "\n",
    "num_inputs = 2  # 输入特征个数\n",
    "num_examples = 1000 # 样本数\n",
    "\n",
    "# 真实的权重w 和偏差 b\n",
    "true_w = nd.array([2,-3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "# 生成输入项\n",
    "features = nd.random.normal(scale = 1,shape = (num_examples,num_inputs))\n",
    "\n",
    "labels = nd.dot(features,true_w.T) + b\n",
    "labels += nd.random.normal(scale = 0.01,shape = labels.shape)\n",
    "\n",
    "######################################## 2、读取数据\n",
    "# 使用 gluon的data包读取数据\n",
    "from mxnet.gluon import data as gdata \n",
    "\n",
    "batch_size = 10 \n",
    "# 将训练数据分成特征和标签组合 \n",
    "dataset = gdata.ArrayDataset(features,labels)\n",
    "dataset[0]\n",
    "\n",
    "# 随机读取小批量\n",
    "data_iter = gdata.DataLoader(dataset,batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "######################################## 3、定义模型\n",
    "from mxnet.gluon import nn \n",
    "\n",
    "# Sequential 实例可以看作是串联各个层的容器，\n",
    "#在构造模型时，在该容器中添加层 当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入\n",
    "net = nn.Sequential()  \n",
    "\n",
    "# 添加一个全连接层，定义该层的输入个数为1\n",
    "net.add(nn.Dense(1))\n",
    "\n",
    "######################################## 4、初始化模型参数\n",
    "from mxnet import init \n",
    "\n",
    "# init 模块提供了模型参数初始化的各种方法。\n",
    "# init.Normal(sigma = 0.01)) 指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正太分布，偏差参数默认初始为0\n",
    "\n",
    "net.initialize(init.Normal(sigma = 0.01)) \n",
    "\n",
    "######################################## 5、策略：定义损失函数\n",
    "from mxnet.gluon import loss as gloss \n",
    "loss = gloss.L2Loss() # L2范数 即平方损失 \n",
    "\n",
    "######################################## 6、算法：定义优化算法\n",
    "from mxnet import gluon \n",
    "# 创建一个Trainer实例，指定学习率 0.03 的小批量随机梯度下降（sgd)为优化算法。 \n",
    "# 该优化算法将用来迭代 net 实例 所有通过 add 函数嵌套的层所包含的全部参数。这些参数可以通过 collect_params 获取\n",
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.03})\n",
    "\n",
    "\n",
    "######################################## 7、训练模型\n",
    "num_epochs = 3 \n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss( net(X), y)\n",
    "        l.backward()\n",
    "        \n",
    "        # 使用 train.step 来迭代模型参数\n",
    "        # new_w = w - (lr/batch_size)* 求导结果：   param - lr * param.grad / batch_size\n",
    "        \n",
    "        trainer.step(batch_size)  \n",
    "    l = loss(net(features),labels)\n",
    "    \n",
    "    print(\"epoch %d loss %f\" %(epoch,l.mean().asnumpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gluon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**步骤：**\n",
    "\n",
    "1、模型 ： 定义 模型函数 如 $f(x) =  wx + b$ \n",
    "\n",
    "2、权重：初始化权重值，$w,b$\n",
    "\n",
    "3、策略：定义损失函数，如 平方损失（L2范数）或其他： $L(w,b) = \\frac{1}{2} (\\hat{y} - y) ^2$\n",
    "\n",
    "4、算法：定义使损失函数 $L(w,b)$ 极小化的算法，如求导，或其他\n",
    "\n",
    "5、训练模型：定义num_epoch（迭代周期），根据2，3求得使 损失函数 $L(w,b)$ 最小的 权重值 $w^*,b^*$ ,得到输出模型 $\\hat{f}(x) = w^* x + b^*$ 用于预测，并计算最终损失：\n",
    "#### $$loss = \\frac{1}{2} (\\hat{y} - y) ^2 = \\frac{1}{2} (w^* x + b^* - y) ^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
